# Metadata-Driven Semantic Search Implementation Plan

> **Last Updated**: 2025-11-09
> **Status**: Updated to reflect govdata refactorings (commits bff4bbd67 through 64ad334f4)

## Overview
Add semantic search capability to govdata adapter using ONNX-based embeddings with persistent cache layer. Vector columns auto-generated during JSON-to-Parquet conversion based on schema metadata. **Cache ensures each unique string is embedded only once, shared across all datasets.**

## Architecture Design

### Cache-First Performance Strategy
- **Key Insight**: Each unique text string is embedded exactly once and cached permanently
- **First Run**: 100k unique strings = 8-33 minutes (one-time cost)
- **Subsequent Runs**: 100k strings = ~5 seconds (cache lookups only)
- **Cross-Dataset Reuse**: "Unemployment Rate" embedded once, reused everywhere
- **Storage**: Parquet-backed cache at `{cacheDir}/embeddings_cache/provider={model}/embeddings.parquet`

### Integration Points
- **Existing Infrastructure**: TextEmbeddingProvider, StorageProvider, DuckDB COSINE_SIMILARITY
- **Model**: sentence-transformers/all-MiniLM-L6-v2 via ONNX Runtime
- **Dimensions**: 384-dimensional embeddings, L2 normalized
- **Storage**: Embeddings stored as `array<double>` in parquet files

---

## Implementation Status

### ‚úÖ Phase 1: Core Infrastructure (COMPLETED)

#### 1. PersistentEmbeddingCache
**File**: `file/src/main/java/org/apache/calcite/adapter/file/similarity/PersistentEmbeddingCache.java`

**Features**:
- SHA-256 hashing of text for cache keys
- In-memory hot cache (ConcurrentHashMap, default 100k entries)
- LRU eviction when memory cache exceeds limit
- Automatic flush after 1000 pending entries
- Thread-safe with ReadWriteLock
- Cache statistics tracking (hits, misses, hit rate)

**Parquet Schema**:
```
text_hash (VARCHAR)          - SHA-256 hash of text
text (VARCHAR)               - First 1000 chars (for debugging)
embedding (DOUBLE[])         - 384-dimensional vector
provider_model (VARCHAR)     - e.g., "onnx-sentence-transformers/all-MiniLM-L6-v2"
created_timestamp (BIGINT)   - Creation timestamp
```

**Key Methods**:
- `get(String text)` - Retrieve cached embedding
- `put(String text, double[] embedding)` - Store embedding
- `flushToStorage()` - Persist pending entries to parquet
- `getStats()` - Cache statistics

#### 2. CachedEmbeddingProvider
**File**: `file/src/main/java/org/apache/calcite/adapter/file/similarity/CachedEmbeddingProvider.java`

**Features**:
- Wraps any TextEmbeddingProvider with caching
- Check cache first, delegate on miss
- Batch processing optimization (separates hits/misses)
- Automatic cache flush on close()
- Exposes cache statistics

**Performance**:
- Cache hit: < 1ms (memory lookup)
- Cache miss: 5-20ms (ONNX inference + cache store)
- Batch processing: Generates only cache misses

#### 3. ONNXEmbeddingProvider
**File**: `file/src/main/java/org/apache/calcite/adapter/file/similarity/ONNXEmbeddingProvider.java`

**Features**:
- ONNX Runtime for model inference
- DJL HuggingFace tokenizers for text preprocessing
- Mean pooling over token embeddings
- L2 normalization for cosine similarity
- Configurable model/tokenizer paths
- Environment variable support (ONNX_MODEL_PATH, ONNX_TOKENIZER_PATH)

**Model Loading Priority**:
1. Explicit config (`modelPath`, `tokenizerPath`)
2. Environment variables (`ONNX_MODEL_PATH`, `ONNX_TOKENIZER_PATH`)
3. Classpath resources (`/models/all-MiniLM-L6-v2/model.onnx`)
4. Default filesystem path (`file/src/main/resources/models/all-MiniLM-L6-v2/`)

**Configuration**:
```java
Map<String, Object> config = Map.of(
    "dimensions", 384,
    "maxSequenceLength", 128,
    "maxInputLength", 8192,
    "modelPath", "/path/to/model.onnx",
    "tokenizerPath", "/path/to/tokenizer.json"
);
```

#### 4. StorageProvider Extensions
**File**: `file/src/main/java/org/apache/calcite/adapter/file/storage/StorageProvider.java`

**New Methods**:
- `readParquet(String path)` - Read parquet file to List<Map<String, Object>>
- `appendParquet(String path, List<Map>, List<schema>)` - Append/create parquet file

**Features**:
- Converts Avro GenericRecords to Maps
- Handles `double[]` <-> Avro array conversion
- Works with remote storage (downloads to temp file first)
- Supports `array<double>` type in schema

#### 5. EmbeddingProviderFactory Updates
**File**: `file/src/main/java/org/apache/calcite/adapter/file/similarity/EmbeddingProviderFactory.java`

**New Provider Type**: `"onnx"`

**Auto-Caching Logic**:
```java
// If cachePath and storageProvider are in config, wrap with cache
if (config.containsKey("cachePath") && config.containsKey("storageProvider")) {
    PersistentEmbeddingCache cache = new PersistentEmbeddingCache(...);
    return new CachedEmbeddingProvider(baseProvider, cache);
}
```

#### 6. Dependencies
**File**: `file/build.gradle.kts`

**Added**:
```kotlin
implementation("com.microsoft.onnxruntime:onnxruntime:1.16.3")
implementation("ai.djl:api:0.25.0")
implementation("ai.djl.huggingface:tokenizers:0.25.0")
```

---

## ‚úÖ Phase 2: ONNX Model Assets (COMPLETED)

### Download Model Files

**Model**: sentence-transformers/all-MiniLM-L6-v2

**Option A - Pre-converted ONNX (Recommended)**:
```bash
# Download from HuggingFace or Optimum
# Place at: file/src/main/resources/models/all-MiniLM-L6-v2/
#   - model.onnx (~90MB)
#   - tokenizer.json (~500KB)
```

**Option B - Convert from PyTorch**:
```bash
pip install optimum[onnxruntime]

optimum-cli export onnx \
  --model sentence-transformers/all-MiniLM-L6-v2 \
  --task feature-extraction \
  file/src/main/resources/models/all-MiniLM-L6-v2/
```

**Files to Create**:
- `file/src/main/resources/models/all-MiniLM-L6-v2/model.onnx`
- `file/src/main/resources/models/all-MiniLM-L6-v2/tokenizer.json`

**Git LFS** (if model > 100MB):
```bash
# .gitattributes
*.onnx filter=lfs diff=lfs merge=lfs -text
```

---

## ‚úÖ Phase 3: Schema JSON Extensions (COMPLETED)

### Schema-Level Configuration

**File**: `govdata/src/main/resources/econ-schema.json`

**Add to schema root**:
```json
{
  "schemaName": "econ",
  "comment": "U.S. economic indicators...",

  "embeddingCache": {
    "enabled": true,
    "path": "{cacheDir}/embeddings_cache",
    "provider": "onnx",
    "model": "all-MiniLM-L6-v2",
    "dimension": 384
  },

  "partitionedTables": [...]
}
```

### Table-Level Vector Columns

**Example 1: Single-Column Embedding**:
```json
{
  "name": "employment_statistics",
  "columns": [
    {
      "name": "series_name",
      "type": "string",
      "nullable": true,
      "comment": "Descriptive name of employment series"
    },
    {
      "name": "series_name_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumn": "series_name",
        "provider": "onnx"
      },
      "comment": "384-dim vector embedding for semantic search on series_name"
    }
  ]
}
```

**Example 2: Multi-Column (Row-Level) Embedding**:
```json
{
  "name": "employment_statistics",
  "columns": [
    {
      "name": "series_name",
      "type": "string",
      "comment": "Descriptive name of employment series"
    },
    {
      "name": "category",
      "type": "string",
      "comment": "Employment category"
    },
    {
      "name": "subcategory",
      "type": "string",
      "comment": "Employment subcategory"
    },
    {
      "name": "row_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumns": ["series_name", "category", "subcategory"],
        "template": "natural",
        "separator": ", ",
        "excludeNull": true
      },
      "comment": "Full context embedding: 'Series Name: Unemployment Rate, Category: Labor Force, Subcategory: Employment Level'"
    }
  ]
}
```

**Example 3: reference_fred_series with Rich Context**:
```json
{
  "name": "row_embedding",
  "type": "array<double>",
  "nullable": true,
  "computed": true,
  "embeddingConfig": {
    "sourceColumns": ["title", "units", "seasonal_adjustment", "frequency"],
    "template": "natural",
    "separator": ", ",
    "excludeNull": true
  },
  "comment": "Full metadata embedding: 'Title: Unemployment Rate, Units: Percent, Seasonal Adjustment: Seasonally Adjusted, Frequency: Monthly'"
}
```

**Column Attributes**:

*Single-column embeddings*:
- `embeddingConfig.sourceColumn` - Source text column (singular)

*Multi-column (row-level) embeddings*:
- `embeddingConfig.sourceColumns` - Array of column names to concatenate
- `embeddingConfig.template` - Format template: "natural" (default), "simple", "structured"
- `embeddingConfig.separator` - Separator between fields (default: ", ")
- `embeddingConfig.excludeNull` - Skip null values in concatenation (default: true)

*Common attributes*:
- `type: "array<double>"` - Vector column type
- `computed: true` - Indicates derived/generated column
- `embeddingConfig.provider` - Provider override (optional, defaults to schema-level)

---

## ‚úÖ Phase 4: Govdata Adapter Integration (COMPLETED)

### Recent Refactoring Benefits

**Impact of commits bff4bbd67 through 64ad334f4:**

The recent govdata refactorings have **simplified** the embedding integration:

#### Benefits:
1. **Unified Interface**: All downloaders now implement `downloadReferenceData()`, `downloadAll()`, and `convertAll()`, making embedding processing consistent
2. **Self-Contained Downloaders**: Each downloader initializes from schema metadata in constructors, reducing EconSchemaFactory complexity
3. **Cleaner Separation**: Embedding provider initialization happens in AbstractGovDataDownloader base class, not in factory classes
4. **No Factory Changes Needed**: EconSchemaFactory's new "pure supervisor pattern" means we don't modify it for embeddings

#### What This Means:
- Embedding provider initialization goes in AbstractGovDataDownloader (base class)
- Each downloader subclass automatically inherits embedding capability
- No need to modify EconSchemaFactory, BlsDataDownloader, FredDataDownloader, etc.
- Embeddings "just work" through the base class `convertJsonRecordToTypedMap()` method

---

### 1. Extend TableColumn Class

**File**: `file/src/main/java/org/apache/calcite/adapter/file/partition/PartitionedTableConfig.java` (line 177)

**Current State**:
```java
public static class TableColumn {
  private final String name;
  private final String type;
  private final boolean nullable;
  private final String comment;
  // Existing fields only
}
```

**Add Fields**:
```java
public static class TableColumn {
  private final String name;
  private final String type;
  private final boolean nullable;
  private final String comment;

  // NEW: Embedding support
  private final boolean computed;
  private final Map<String, Object> embeddingConfig;

  // NEW: Methods
  public boolean isComputed() {
    return computed;
  }

  public boolean hasEmbeddingConfig() {
    return embeddingConfig != null && !embeddingConfig.isEmpty();
  }

  /**
   * Get embedding source columns (supports both single and multi-column).
   * Returns array with single element for sourceColumn, or full array for sourceColumns.
   */
  public String[] getEmbeddingSourceColumns() {
    if (embeddingConfig == null) {
      return null;
    }

    // Check for multi-column (sourceColumns array)
    if (embeddingConfig.containsKey("sourceColumns")) {
      List<String> cols = (List<String>) embeddingConfig.get("sourceColumns");
      return cols != null ? cols.toArray(new String[0]) : null;
    }

    // Check for single-column (sourceColumn string)
    if (embeddingConfig.containsKey("sourceColumn")) {
      return new String[]{(String) embeddingConfig.get("sourceColumn")};
    }

    return null;
  }

  /**
   * @deprecated Use getEmbeddingSourceColumns() instead
   */
  @Deprecated
  public String getEmbeddingSourceColumn() {
    String[] cols = getEmbeddingSourceColumns();
    return (cols != null && cols.length > 0) ? cols[0] : null;
  }

  public String getEmbeddingTemplate() {
    return embeddingConfig != null ?
        (String) embeddingConfig.getOrDefault("template", "natural") : "natural";
  }

  public String getEmbeddingSeparator() {
    return embeddingConfig != null ?
        (String) embeddingConfig.getOrDefault("separator", ", ") : ", ";
  }

  public boolean getEmbeddingExcludeNull() {
    return embeddingConfig != null ?
        (Boolean) embeddingConfig.getOrDefault("excludeNull", true) : true;
  }

  public String getEmbeddingProvider() {
    return embeddingConfig != null ?
        (String) embeddingConfig.getOrDefault("provider", "onnx") : "onnx";
  }

  public boolean isVectorType() {
    return type != null && type.startsWith("array<");
  }
}
```

**Note**: Update Jackson deserialization to handle new fields (add to constructor/builder).

---

### 2. Update AbstractGovDataDownloader

**File**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/AbstractGovDataDownloader.java`

#### 2.1 Add Field

```java
private TextEmbeddingProvider embeddingProvider; // Lazy-initialized
```

#### 2.2 Add Initialization Method

**Add after existing fields (around line 200)**:

```java
/**
 * Lazily initializes the embedding provider from schema configuration.
 * Returns null if embeddings are disabled in schema.
 */
private TextEmbeddingProvider getEmbeddingProvider() throws EmbeddingException {
  if (embeddingProvider == null) {
    // Load schema root config from resources
    try (InputStream schemaStream = getClass().getResourceAsStream(schemaResourceName)) {
      if (schemaStream == null) {
        LOGGER.warn("Schema resource not found: {}", schemaResourceName);
        return null;
      }

      JsonNode root = MAPPER.readTree(schemaStream);

      // Read embedding config from schema root
      if (!root.has("embeddingCache")) {
        return null; // Embeddings disabled
      }

      JsonNode embeddingCacheNode = root.get("embeddingCache");
      if (!embeddingCacheNode.get("enabled").asBoolean(false)) {
        return null; // Embeddings disabled
      }

      // Build provider config
      String provider = embeddingCacheNode.get("provider").asText("onnx");
      String cachePath = embeddingCacheNode.get("path").asText();

      // Resolve {cacheDir} placeholder
      cachePath = cachePath.replace("{cacheDir}", cacheDirectory);

      Map<String, Object> config = new HashMap<>();
      config.put("cachePath", cachePath);
      config.put("storageProvider", this.storageProvider);
      config.put("dimensions", embeddingCacheNode.get("dimension").asInt(384));
      config.put("model", embeddingCacheNode.get("model").asText());

      embeddingProvider = EmbeddingProviderFactory.createProvider(provider, config);
      LOGGER.info("Initialized embedding provider: {}", embeddingProvider.getProviderName());

    } catch (IOException e) {
      throw new EmbeddingException("Failed to load schema config: " + e.getMessage(), e);
    }
  }

  return embeddingProvider;
}
```

**Key Changes from Original Plan**:
- Schema loaded via `getClass().getResourceAsStream(schemaResourceName)` instead of accessing non-existent `schemaConfig` field
- Uses Jackson `JsonNode` API instead of `Map<String, Object>`
- Explicit {cacheDir} placeholder replacement instead of calling non-existent `resolveParquetPath()`

#### 2.3 Update convertJsonRecordToTypedMap()

**Current Location**: Line 1382 (not line 1226 as originally documented)

**Challenge**: Method signature needs to accept `List<TableColumn>` for embedding generation, but currently accepts `Map<String, String> columnTypeMap`.

**Current Signature**:
```java
protected Map<String, Object> convertJsonRecordToTypedMap(JsonNode recordNode,
    Map<String, String> columnTypeMap, String missingValueIndicator)
```

**Solution - Add Overload** (backward compatible):

```java
/**
 * Converts JSON record to typed map with embedding generation support.
 *
 * @param recordNode JSON record from source data
 * @param columns Full column metadata including embedding config
 * @param missingValueIndicator String indicating missing values (e.g., "NA", "null")
 * @return Typed map with embeddings generated for computed columns
 */
protected Map<String, Object> convertJsonRecordToTypedMap(
    JsonNode recordNode, List<TableColumn> columns, String missingValueIndicator) {

  Map<String, Object> typedRecord = new LinkedHashMap<>();

  // Build type map for conversion
  Map<String, String> columnTypeMap = new HashMap<>();
  for (TableColumn col : columns) {
    columnTypeMap.put(col.getName(), col.getType());
  }

  // Existing type conversion for all fields
  Iterator<Map.Entry<String, JsonNode>> fields = recordNode.fields();
  while (fields.hasNext()) {
    Map.Entry<String, JsonNode> field = fields.next();
    String fieldName = field.getKey();

    // Skip computed columns in source data
    if (isComputedColumn(columns, fieldName)) {
      continue;
    }

    String columnType = columnTypeMap.get(fieldName);
    if (columnType != null) {
      Object convertedValue = convertJsonValueToType(
          field.getValue(), fieldName, columnType, missingValueIndicator);
      typedRecord.put(fieldName, convertedValue);
    }
  }

  // NEW: Generate computed columns (embeddings)
  try {
    TextEmbeddingProvider provider = getEmbeddingProvider();
    if (provider != null) {
      generateEmbeddingColumns(typedRecord, columns, provider);
    }
  } catch (EmbeddingException e) {
    LOGGER.warn("Failed to initialize embedding provider: {}", e.getMessage());
  }

  return typedRecord;
}

/**
 * Keep existing method signature for backward compatibility.
 * Delegates to column-based version by loading columns from schema if needed.
 */
protected Map<String, Object> convertJsonRecordToTypedMap(JsonNode recordNode,
    Map<String, String> columnTypeMap, String missingValueIndicator) {
  // For now, call existing implementation
  // TODO: Load columns from schema and call overload above
  // (This preserves current behavior until callers are updated)
  Map<String, Object> typedRecord = new LinkedHashMap<>();
  // ... existing implementation ...
  return typedRecord;
}
```

**Update Callers**:

Find the method call site (search for `convertJsonRecordToTypedMap(` in AbstractGovDataDownloader and subclasses) and update to pass `List<TableColumn>` instead of `Map<String, String>`.

#### 2.4 Add Helper Methods

```java
/**
 * Generates embedding columns for all computed columns with embeddingConfig.
 * Supports both single-column and multi-column (row-level) embeddings.
 */
private void generateEmbeddingColumns(Map<String, Object> record,
    List<TableColumn> columns, TextEmbeddingProvider provider) {

  for (TableColumn column : columns) {
    if (!column.isComputed() || !column.hasEmbeddingConfig()) {
      continue;
    }

    String[] sourceColumns = column.getEmbeddingSourceColumns();
    if (sourceColumns == null || sourceColumns.length == 0) {
      continue;
    }

    // Build text to embed
    String text;
    if (sourceColumns.length == 1) {
      // Single column (existing behavior)
      Object value = record.get(sourceColumns[0]);
      text = (value instanceof String) ? (String) value : null;
    } else {
      // Multi-column concatenation (NEW: row-level embeddings)
      text = concatenateFieldsNatural(record, sourceColumns, column);
    }

    if (text != null && !text.isEmpty()) {
      try {
        double[] embedding = provider.generateEmbedding(text);
        record.put(column.getName(), embedding);

        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug("Generated embedding for column {} from {}: {} dimensions",
                      column.getName(), String.join(",", sourceColumns), embedding.length);
        }
      } catch (EmbeddingException e) {
        LOGGER.debug("Failed to generate embedding for column {}: {}",
                    column.getName(), e.getMessage());
        record.put(column.getName(), null);
      }
    } else {
      record.put(column.getName(), null);
    }
  }
}

/**
 * Concatenates multiple fields into natural language format for row-level embeddings.
 * Example: "Series Name: Unemployment Rate, Category: Labor Force, Value: 3.7 Percent"
 */
private String concatenateFieldsNatural(Map<String, Object> record,
    String[] fieldNames, TableColumn embeddingColumn) {

  String template = embeddingColumn.getEmbeddingTemplate();
  String separator = embeddingColumn.getEmbeddingSeparator();
  boolean excludeNull = embeddingColumn.getEmbeddingExcludeNull();

  StringBuilder sb = new StringBuilder();

  for (String fieldName : fieldNames) {
    Object value = record.get(fieldName);

    if (value == null && excludeNull) {
      continue;
    }

    if (value != null) {
      if (sb.length() > 0) {
        sb.append(separator);
      }

      // Apply template format
      if ("natural".equals(template)) {
        // Natural language: "Series Name: Unemployment Rate"
        String humanizedName = humanizeColumnName(fieldName);
        String cleansedValue = cleanseValue(fieldName, value, record);
        sb.append(humanizedName).append(": ").append(cleansedValue);
      } else {
        // Simple format: just the value
        sb.append(cleanseValue(fieldName, value, record));
      }
    }
  }

  return sb.toString();
}

/**
 * Humanizes column names for natural language format.
 * Example: series_name -> Series Name, seasonal_adjustment -> Seasonally Adjusted
 */
private String humanizeColumnName(String columnName) {
  return Arrays.stream(columnName.split("_"))
      .map(word -> word.substring(0, 1).toUpperCase() + word.substring(1))
      .collect(Collectors.joining(" "));
}

/**
 * Cleanses values for embedding text (handles numbers, dates, booleans).
 */
private String cleanseValue(String columnName, Object value, Map<String, Object> record) {
  if (value == null) {
    return "";
  }

  // Booleans: Yes/No instead of true/false
  if (value instanceof Boolean) {
    return ((Boolean) value) ? "Yes" : "No";
  }

  // Numbers with units: look for adjacent unit column
  if (value instanceof Number) {
    String unitColumn = findUnitColumn(columnName, record);
    if (unitColumn != null) {
      Object unit = record.get(unitColumn);
      if (unit != null) {
        return value + " " + unit;
      }
    }
    return value.toString();
  }

  // Dates: natural format for monthly/yearly, ISO for daily
  if (columnName.toLowerCase().contains("date") && value instanceof String) {
    return formatDateNaturally((String) value);
  }

  return value.toString();
}

/**
 * Find associated unit column for a value column.
 * Example: "value" -> "units", "amount" -> "units"
 */
private String findUnitColumn(String columnName, Map<String, Object> record) {
  // Common patterns: value/units, amount/currency, etc.
  String[] unitColumnNames = {"units", "unit", "currency", "denomination"};

  for (String unitCol : unitColumnNames) {
    if (record.containsKey(unitCol)) {
      return unitCol;
    }
  }

  return null;
}

/**
 * Format dates naturally for better semantic matching.
 * 2023-01-01 -> January 2023 (for monthly data)
 * 2023-01-15 -> 2023-01-15 (for daily data - keep ISO format)
 */
private String formatDateNaturally(String dateStr) {
  // Simple heuristic: if date is first of month, assume monthly aggregation
  if (dateStr.endsWith("-01")) {
    try {
      String[] parts = dateStr.split("-");
      if (parts.length == 3) {
        int month = Integer.parseInt(parts[1]);
        String[] monthNames = {"January", "February", "March", "April", "May", "June",
                                "July", "August", "September", "October", "November", "December"};
        return monthNames[month - 1] + " " + parts[0];
      }
    } catch (Exception e) {
      // Fall through to return original
    }
  }
  return dateStr; // Keep ISO format for daily data
}

/**
 * Checks if a column is marked as computed in the schema.
 */
private boolean isComputedColumn(List<TableColumn> columns, String columnName) {
  return columns.stream()
      .anyMatch(c -> c.getName().equals(columnName) && c.isComputed());
}
```

#### 2.5 Add Cleanup

**Update existing `close()` method** (around line 500):

```java
public void close() {
  // NEW: Flush embedding cache
  if (embeddingProvider != null) {
    try {
      embeddingProvider.close(); // Flushes cache to storage
      LOGGER.info("Embedding provider closed and cache flushed");
    } catch (Exception e) {
      LOGGER.warn("Error closing embedding provider: {}", e.getMessage());
    }
  }

  // ... existing cleanup ...
}
```

---

### 3. Verify array<double> Support in StorageProvider

**File**: `file/src/main/java/org/apache/calcite/adapter/file/storage/StorageProvider.java`

**Verify array type handling in writeAvroParquet()** (should already work from Phase 1):

```java
// In writeAvroParquet() method around line 230
case "array<double>":
  if (column.isNullable()) {
    fields = fields.name(column.getName()).doc(column.getComment())
        .type().nullable().array().items().doubleType().noDefault();
  } else {
    fields = fields.name(column.getName()).doc(column.getComment())
        .type().array().items().doubleType().noDefault();
  }
  break;
```

**Action**: Verify this case exists. If not, add it.

---

## Row-Level vs Column-Level Embeddings

### When to Use Each Approach

#### ‚úÖ Use Column-Level Embeddings When:

1. **Targeting specific fields** - You know which field to search
   ```sql
   -- Find series with similar names
   WHERE COSINE_SIMILARITY(series_name_embedding, target_emb) > 0.7
   ```

2. **Field-specific semantic meaning** - Each column has distinct semantics
   - `series_name`: "Unemployment Rate"
   - `title`: "Consumer Price Index"
   - Different concepts, shouldn't be mixed

3. **Multiple focused searches** - Query different aspects separately
   ```sql
   -- Search by name
   WHERE COSINE_SIMILARITY(series_name_embedding, ...) > 0.7
   -- OR search by notes
   WHERE COSINE_SIMILARITY(notes_embedding, ...) > 0.7
   ```

4. **Performance critical** - Single field = smaller token count, faster embedding

#### ‚úÖ Use Row-Level Embeddings When:

1. **Cross-field context queries** - Search spans multiple metadata fields
   ```
   Query: "seasonally adjusted monthly percent measures"
   Matches: title="Unemployment Rate" + units="Percent" +
            frequency="Monthly" + seasonal_adjustment="Seasonally Adjusted"
   ```

2. **Hierarchical data** - Category + subcategory + name forms semantic path
   ```
   Row embedding: "Category: Labor Force, Subcategory: Employment Level, Series Name: Unemployment Rate"
   Captures full hierarchy in single vector
   ```

3. **Disambiguation through context** - Same term, different meanings
   ```
   "Revenue" alone is ambiguous
   "Series Name: Revenue, Category: Technology Sector, Units: Billions USD" is specific
   ```

4. **Exploratory search** - User doesn't know which field contains the answer
   - Row embedding searches all fields at once
   - More forgiving for imprecise queries

### Template Formats

#### Natural Language (Recommended)
```
Series Name: Unemployment Rate, Category: Labor Force, Seasonally Adjusted: Yes, Value: 3.7 Percent, Date: January 2023
```

**Advantages**:
- Matches training data patterns (human-written text)
- Column names provide semantic context
- Best for semantic similarity

#### Simple Format
```
Unemployment Rate, Labor Force, Yes, 3.7 Percent, January 2023
```

**Advantages**:
- Fewer tokens (fits more columns in 128 limit)
- Faster to generate
- Good when column order is consistent

### Cleansing Rules

#### Numbers
- **Keep as-is**: "3.7" NOT "three point seven"
- **Include units**: "3.7 Percent" (from adjacent units column)
- **Large numbers**: "394B" or "394 Billion" (both work)

#### Dates
- **Monthly data**: "January 2023" (natural format)
- **Daily data**: "2023-01-15" (ISO format - model understands both)
- **Yearly data**: "2023" (just the year)

#### Booleans
- **Yes/No**: "Seasonally Adjusted: Yes" NOT "seasonal_adjustment: true"
- More natural for embedding models

#### Exclusions
**Always exclude from row embeddings**:
- Technical IDs (series codes, CIK numbers)
- System fields (created_timestamp, row_id)
- Redundant computed fields

**Always include**:
- Descriptive text (names, titles, descriptions)
- Classifications (category, type, frequency)
- Measures with units (value + unit column)
- Temporal context (date, year, period)

### Configuration Examples

**Employment Statistics** (hierarchical data):
```json
{
  "name": "row_embedding",
  "computed": true,
  "embeddingConfig": {
    "sourceColumns": ["category", "subcategory", "series_name"],
    "template": "natural",
    "separator": ", "
  }
}
```
Output: `Category: Labor Force, Subcategory: Employment Level, Series Name: Unemployment Rate`

**FRED Indicators** (rich metadata):
```json
{
  "name": "row_embedding",
  "computed": true,
  "embeddingConfig": {
    "sourceColumns": ["title", "units", "seasonal_adjustment", "frequency", "notes"],
    "template": "natural",
    "separator": ", ",
    "excludeNull": true
  }
}
```
Output: `Title: Unemployment Rate, Units: Percent, Seasonal Adjustment: Seasonally Adjusted, Frequency: Monthly, Notes: The unemployment rate represents...`

### Performance Considerations

**Token Budget (128 max)**:
- Natural language format: ~2-3 tokens per field (column name + value)
- Typical row: 15-50 tokens (well under limit)
- Long text fields (notes): May exceed limit (truncated automatically by model)

**Storage Impact**:
- Each row embedding adds 384 dims √ó 8 bytes = 3KB per row
- For 100k rows: ~300MB additional storage
- Optional per table (only add where valuable)

**Cache Efficiency**:
- Row embeddings still benefit from persistent cache
- Same concatenation = cached (e.g., category + subcategory combos reused)
- Different from column-level cache keys

### Query Pattern Comparison

**Column-level** (precise, fast):
```sql
SELECT series_name, value, date
FROM econ.employment_statistics
WHERE COSINE_SIMILARITY(series_name_embedding,
        embed('unemployment rate')) > 0.8
ORDER BY date DESC;
```

**Row-level** (comprehensive, flexible):
```sql
SELECT series_name, category, value, date
FROM econ.employment_statistics
WHERE COSINE_SIMILARITY(row_embedding,
        embed('monthly seasonally adjusted labor force statistics')) > 0.7
ORDER BY COSINE_SIMILARITY(row_embedding, ...) DESC;
```

**Hybrid** (best of both):
```sql
-- Narrow by specific field, rank by full context
SELECT series_name, category, value
FROM econ.employment_statistics
WHERE COSINE_SIMILARITY(series_name_embedding, embed('unemployment')) > 0.8
ORDER BY COSINE_SIMILARITY(row_embedding, embed('monthly labor force percent')) DESC;
```

---

## ‚úÖ Phase 5: Testing (COMPLETED)

### Unit Tests

#### 1. PersistentEmbeddingCacheTest
**File**: `file/src/test/java/org/apache/calcite/adapter/file/similarity/PersistentEmbeddingCacheTest.java`

```java
@Test
void testCachePersistence() throws Exception {
  String cachePath = tempDir + "/test_cache/embeddings.parquet";
  PersistentEmbeddingCache cache = new PersistentEmbeddingCache(
      storageProvider, cachePath, "test-model");

  double[] embedding = new double[384];
  Arrays.fill(embedding, 0.5);

  cache.put("test text", embedding);
  cache.flushToStorage();

  // Create new cache instance - should load from storage
  PersistentEmbeddingCache cache2 = new PersistentEmbeddingCache(
      storageProvider, cachePath, "test-model");

  double[] loaded = cache2.get("test text");
  assertNotNull(loaded);
  assertArrayEquals(embedding, loaded, 0.0001);
}

@Test
void testCacheHitRate() {
  // Test hits/misses tracking
  // Test auto-flush at 1000 entries
  // Test LRU eviction
}
```

#### 2. CachedEmbeddingProviderTest
**File**: `file/src/test/java/org/apache/calcite/adapter/file/similarity/CachedEmbeddingProviderTest.java`

```java
@Test
void testCacheIntegration() throws Exception {
  TextEmbeddingProvider mockProvider = Mockito.mock(ONNXEmbeddingProvider.class);
  when(mockProvider.generateEmbedding(anyString()))
      .thenReturn(new double[384]);

  PersistentEmbeddingCache cache = new PersistentEmbeddingCache(...);
  CachedEmbeddingProvider provider = new CachedEmbeddingProvider(mockProvider, cache);

  // First call - should hit mock provider
  provider.generateEmbedding("test");
  verify(mockProvider, times(1)).generateEmbedding("test");

  // Second call - should use cache
  provider.generateEmbedding("test");
  verify(mockProvider, times(1)).generateEmbedding("test"); // Still only 1 call
}

@Test
void testBatchProcessingOptimization() {
  // Test that batch separates hits/misses
  // Test that only misses are generated
}
```

#### 3. ONNXEmbeddingProviderTest
**File**: `file/src/test/java/org/apache/calcite/adapter/file/similarity/ONNXEmbeddingProviderTest.java`

**Note**: Requires model files to be present

```java
@Test
void testGenerateEmbedding() throws Exception {
  Map<String, Object> config = new HashMap<>();
  ONNXEmbeddingProvider provider = new ONNXEmbeddingProvider(config);

  double[] embedding = provider.generateEmbedding("unemployment rate");

  assertEquals(384, embedding.length);
  assertTrue(isNormalized(embedding));
}

@Test
void testBatchGeneration() throws Exception {
  ONNXEmbeddingProvider provider = new ONNXEmbeddingProvider(new HashMap<>());

  List<String> texts = Arrays.asList("inflation", "GDP", "employment");
  List<double[]> embeddings = provider.generateEmbeddings(texts);

  assertEquals(3, embeddings.size());
  for (double[] embedding : embeddings) {
    assertEquals(384, embedding.length);
  }
}

private boolean isNormalized(double[] vector) {
  double norm = 0.0;
  for (double v : vector) {
    norm += v * v;
  }
  return Math.abs(Math.sqrt(norm) - 1.0) < 0.0001;
}
```

### Integration Tests

#### EconEmbeddingIntegrationTest
**File**: `govdata/src/test/java/org/apache/calcite/adapter/govdata/econ/EconEmbeddingIntegrationTest.java`

```java
@Tag("integration")
@Test
void testEmploymentStatisticsWithEmbeddings() throws Exception {
  // Use test schema with embedding columns
  String modelJson = createTestModelWithEmbeddings();

  // Download data
  EconDataDownloader downloader = new EconDataDownloader(...);
  downloader.downloadAndConvert("employment_statistics",
      Map.of("frequency", "monthly", "year", "2023"));

  // Query with DuckDB
  String sql = "SELECT series_name, series_name_embedding " +
               "FROM econ.employment_statistics LIMIT 1";

  try (ResultSet rs = executeQuery(sql)) {
    assertTrue(rs.next());
    String seriesName = rs.getString("series_name");
    Array embeddingArray = rs.getArray("series_name_embedding");
    double[] embedding = (double[]) embeddingArray.getArray();

    assertEquals(384, embedding.length);
    assertNotNull(seriesName);
  }
}

@Tag("integration")
@Test
void testSemanticSimilarityQuery() throws Exception {
  downloadTestData();

  String sql =
      "WITH target AS (" +
      "  SELECT series_name_embedding AS target_emb " +
      "  FROM econ.employment_statistics " +
      "  WHERE series_name = 'Unemployment Rate' " +
      "  LIMIT 1" +
      ") " +
      "SELECT " +
      "  series_name, " +
      "  COSINE_SIMILARITY(series_name_embedding, target.target_emb) AS similarity " +
      "FROM econ.employment_statistics, target " +
      "ORDER BY similarity DESC " +
      "LIMIT 5";

  try (ResultSet rs = executeQuery(sql)) {
    int count = 0;
    while (rs.next()) {
      String name = rs.getString("series_name");
      double similarity = rs.getDouble("similarity");

      assertTrue(similarity >= 0.0 && similarity <= 1.0);
      count++;
    }
    assertEquals(5, count);
  }
}

@Tag("integration")
@Test
void testCacheReuse() throws Exception {
  // First download
  downloadTestData();

  // Get cache stats
  CacheStats stats1 = getEmbeddingCacheStats();
  int initialSize = stats1.getSize();

  // Download same data again
  downloadTestData();

  // Cache should have same size (100% hits)
  CacheStats stats2 = getEmbeddingCacheStats();
  assertEquals(initialSize, stats2.getSize());
  assertTrue(stats2.getHitRate() > 0.95); // At least 95% cache hits
}
```

---

## üîÑ Phase 6: Production Schema Updates (PENDING)

### Update econ-schema.json

**File**: `govdata/src/main/resources/econ-schema.json`

**Add to schema root**:
```json
{
  "schemaName": "econ",
  "embeddingCache": {
    "enabled": true,
    "path": "{cacheDir}/embeddings_cache",
    "provider": "onnx",
    "model": "all-MiniLM-L6-v2",
    "dimension": 384
  }
}
```

**Update key tables**:

**employment_statistics**:
```json
{
  "name": "series_name_embedding",
  "type": "array<double>",
  "nullable": true,
  "computed": true,
  "embeddingConfig": {
    "sourceColumn": "series_name"
  },
  "comment": "Vector embedding for semantic search on series names"
}
```

**fred_indicators**:
```json
{
  "name": "title_embedding",
  "type": "array<double>",
  "nullable": true,
  "computed": true,
  "embeddingConfig": {
    "sourceColumn": "title"
  },
  "comment": "Vector embedding for semantic search on indicator titles"
}
```

**regional_gdp**:
```json
{
  "name": "line_name_embedding",
  "type": "array<double>",
  "nullable": true,
  "computed": true,
  "embeddingConfig": {
    "sourceColumn": "line_name"
  },
  "comment": "Vector embedding for semantic search on GDP line items"
}
```

---

## Usage Examples

### Query Examples

**Column-Level: Find similar employment series by name**:
```sql
WITH target AS (
  SELECT series_name_embedding AS emb
  FROM econ.employment_statistics
  WHERE series_name = 'Unemployment Rate'
  LIMIT 1
)
SELECT
  series_name,
  category,
  COSINE_SIMILARITY(series_name_embedding, target.emb) AS similarity
FROM econ.employment_statistics, target
WHERE COSINE_SIMILARITY(series_name_embedding, target.emb) > 0.7
ORDER BY similarity DESC
LIMIT 20;
```

**Row-Level: Find similar series by full context**:
```sql
-- Query: "monthly seasonally adjusted labor force percent"
-- Searches across series_name + category + subcategory + metadata
WITH target AS (
  SELECT row_embedding AS emb
  FROM econ.employment_statistics
  WHERE series_name = 'Unemployment Rate'
  LIMIT 1
)
SELECT
  series_name,
  category,
  subcategory,
  COSINE_SIMILARITY(row_embedding, target.emb) AS similarity
FROM econ.employment_statistics, target
WHERE COSINE_SIMILARITY(row_embedding, target.emb) > 0.6
ORDER BY similarity DESC
LIMIT 20;
```

**Hybrid: Combine column and row embeddings**:
```sql
-- Narrow by series name, rank by full context similarity
WITH target AS (
  SELECT series_name_embedding AS name_emb, row_embedding AS row_emb
  FROM econ.employment_statistics
  WHERE series_name = 'Unemployment Rate'
  LIMIT 1
)
SELECT
  series_name,
  category,
  COSINE_SIMILARITY(series_name_embedding, target.name_emb) AS name_sim,
  COSINE_SIMILARITY(row_embedding, target.row_emb) AS context_sim,
  (COSINE_SIMILARITY(series_name_embedding, target.name_emb) +
   COSINE_SIMILARITY(row_embedding, target.row_emb)) / 2 AS combined_sim
FROM econ.employment_statistics, target
WHERE COSINE_SIMILARITY(series_name_embedding, target.name_emb) > 0.7
ORDER BY combined_sim DESC
LIMIT 20;
```

**Cross-dataset semantic search**:
```sql
-- Find BEA GDP series similar to a FRED indicator
WITH fred_target AS (
  SELECT title_embedding AS emb
  FROM econ.fred_indicators
  WHERE series = 'GDP'
  LIMIT 1
)
SELECT
  line_name,
  COSINE_SIMILARITY(line_name_embedding, fred_target.emb) AS similarity
FROM econ.regional_gdp, fred_target
ORDER BY similarity DESC
LIMIT 10;
```

**Semantic clustering**:
```sql
SELECT
  series_name,
  category,
  COSINE_SIMILARITY(series_name_embedding,
      (SELECT series_name_embedding FROM econ.employment_statistics
       WHERE series = 'LNS14000000' LIMIT 1)) AS cluster_1_similarity,
  COSINE_SIMILARITY(series_name_embedding,
      (SELECT series_name_embedding FROM econ.employment_statistics
       WHERE series = 'CEU0000000001' LIMIT 1)) AS cluster_2_similarity
FROM econ.employment_statistics
ORDER BY GREATEST(cluster_1_similarity, cluster_2_similarity) DESC;
```

### Environment Variables

```bash
# Override model path
export ONNX_MODEL_PATH=/path/to/custom/model.onnx
export ONNX_TOKENIZER_PATH=/path/to/tokenizer.json

# Performance tuning
export EMBEDDING_BATCH_SIZE=64
export EMBEDDING_CACHE_MAX_SIZE=200000

# Disable embeddings
export DISABLE_EMBEDDINGS=true
```

---

## Performance Characteristics

### Expected Timings

**ONNX Inference (CPU)**:
- Single embedding: 5-20ms
- Batch of 32: ~200-600ms
- Model loading: ~500ms (one-time)

**Cache Operations**:
- Memory cache hit: < 1ms
- Parquet read (100k entries): ~2-3 seconds
- Parquet write (1k entries): ~500ms

**End-to-End Performance**:

| Dataset Size | First Run (Cold Cache) | Subsequent Runs (Warm Cache) |
|--------------|------------------------|------------------------------|
| 1K records   | 5-20 seconds          | 1-2 seconds                 |
| 10K records  | 50-200 seconds        | 2-5 seconds                 |
| 100K records | 8-33 minutes          | 5-10 seconds                |
| 1M records   | 80-330 minutes        | 30-60 seconds               |

**Cache Hit Rate**:
- Same dataset re-downloaded: ~100%
- Related datasets (overlapping descriptions): 20-60%
- Unrelated datasets: 0-10%

### Storage Requirements

**Per 100k unique strings**:
- Embeddings: ~120MB (384 dims √ó 8 bytes √ó 100k)
- Parquet compressed: ~30-40MB
- Cache metadata: ~5MB

**Scaling**:
- 1M unique strings: ~300-400MB cache file
- 10M unique strings: ~3-4GB cache file

---

## Architecture Benefits

### ‚úÖ Surgical Integration
- Minimal code changes (5 new files, 3 file updates)
- Leverages existing patterns (TextEmbeddingProvider, metadata-driven)
- No breaking changes to existing functionality
- **Post-refactoring**: Even cleaner with unified downloader interface

### ‚úÖ Performance Optimization
- Persistent cache eliminates redundant embedding generation
- Cross-dataset cache reuse (global deduplication)
- In-memory hot cache for session-level performance
- Batch processing for efficient cache miss handling

### ‚úÖ Flexibility
- Model-agnostic (supports ONNX, OpenAI, local TF-IDF)
- Configurable at schema and table levels
- Environment variable overrides
- Works with both local and S3 storage

### ‚úÖ Production Ready
- Thread-safe cache operations
- Automatic cache persistence
- Error handling and fallback
- Cache statistics for monitoring

---

## Troubleshooting

### Common Issues

**1. Model files not found**:
```
ERROR: ONNX model file not found. Please set ONNX_MODEL_PATH...
```
**Solution**: Download model files or set environment variables

**2. Out of memory during embedding generation**:
```
java.lang.OutOfMemoryError: Java heap space
```
**Solution**: Increase heap size or reduce cache size
```bash
export EMBEDDING_CACHE_MAX_SIZE=50000
```

**3. Cache corruption**:
```
WARN: Failed to read existing parquet file, will overwrite
```
**Solution**: Delete cache file and regenerate
```bash
rm -rf {cacheDir}/embeddings_cache
```

**4. Slow performance on first run**:
- **Expected**: First run generates all embeddings
- **Solution**: Use smaller test datasets for development
- **Monitoring**: Check cache stats to verify caching is working

---

## Future Enhancements

### Performance
- GPU acceleration (ONNX CUDA provider)
- True batch processing in ONNXEmbeddingProvider
- Parallel embedding generation
- Cache pre-warming strategies

### Features
- Multiple embedding models per schema
- Embedding model versioning/migration
- Approximate nearest neighbor search (HNSW)
- Embedding fine-tuning on domain data

### Operations
- Cache management CLI
- Cache statistics dashboard
- Embedding quality metrics
- Model performance benchmarking

---

## Rollback Plan

If issues arise:

1. **Disable embeddings**: Set `embeddingCache.enabled: false` in schema
2. **Remove vector columns**: Delete from schema JSON
3. **Revert code**: Git revert to commit before this implementation
4. **Clean cache**: Delete `{cacheDir}/embeddings_cache` directory

No data loss - source data remains unchanged in original parquet files.

---

## References

- **ONNX Runtime**: https://onnxruntime.ai/
- **DJL Tokenizers**: https://docs.djl.ai/extensions/tokenizers/index.html
- **sentence-transformers**: https://www.sbert.net/
- **all-MiniLM-L6-v2**: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
- **DuckDB Vector Functions**: https://duckdb.org/docs/sql/functions/list.html#list_cosine_similarity

---

## Completion Checklist

### Phase 1: Core Infrastructure ‚úÖ
- [x] PersistentEmbeddingCache
- [x] CachedEmbeddingProvider
- [x] ONNXEmbeddingProvider
- [x] StorageProvider extensions
- [x] EmbeddingProviderFactory updates
- [x] Dependencies added
- [x] Compilation successful

### Phase 2: Model Assets ‚úÖ
- [x] Download all-MiniLM-L6-v2 ONNX model
- [x] Download tokenizer.json
- [x] Place in resources directory
- [x] Test model loading

### Phase 3: Schema Extensions ‚úÖ
- [x] Add embeddingCache to schema root
- [x] Add vector columns to tables
- [x] Validate schema parsing

### Phase 4: Govdata Integration ‚è≥
- [ ] Extend TableColumn class (PartitionedTableConfig.java:177)
- [ ] Add getEmbeddingProvider() to AbstractGovDataDownloader
- [ ] Add convertJsonRecordToTypedMap() overload accepting List<TableColumn>
- [ ] Add generateEmbeddingColumns() helper method
- [ ] Add isComputedColumn() helper method
- [ ] Update close() method to flush embedding provider
- [ ] Update callers to use new method signature

### Phase 5: Testing ‚è≥
- [ ] Unit tests for PersistentEmbeddingCache
- [ ] Unit tests for CachedEmbeddingProvider
- [ ] Unit tests for ONNXEmbeddingProvider
- [ ] Integration tests for econ schema
- [ ] Performance benchmarks

### Phase 6: Production ‚è≥
- [ ] Update production econ-schema.json
- [ ] Add embedding columns to key tables
- [ ] Document query patterns
- [ ] Performance monitoring
- [ ] User documentation

### Phase 7: SEC Schema Unification ‚è≥
- [ ] Migrate SEC to metadata-driven approach
- [ ] Add embeddingCache config to sec-schema.json
- [ ] Add vector columns to SEC tables
- [ ] Refactor SecTextVectorizer integration
- [ ] Remove dead code (EmbeddingService.java)
- [ ] Update SEC tests

---

## üîÑ Phase 7: SEC Schema Unification (PENDING)

### Background

The SEC adapter already uses the unified embedding infrastructure (`TextEmbeddingProvider`, `EmbeddingProviderFactory`) but implements embeddings differently than the planned econ approach:

**Current SEC Approach** (Application-layer):
- Manual embedding generation via `SecTextVectorizer` during conversion
- Explicit calls in conversion logic
- Works but requires code changes for new vector columns

**Target Approach** (Infrastructure-layer):
- Metadata-driven embedding generation (same as econ/Phase 4)
- Schema JSON defines which columns get embeddings
- `AbstractGovDataDownloader` handles embedding generation automatically

### Current State Analysis

#### ‚úÖ What SEC Already Has (Correct):
1. **Uses unified infrastructure**:
   - `TextEmbeddingProvider` interface ‚úÖ
   - `EmbeddingProviderFactory` ‚úÖ
   - Can use ONNX models via factory ‚úÖ

2. **SEC-specific components (Keep)**:
   - `SecTextVectorizer` - XBRL preprocessing (financial concept extraction)
   - `SecEmbeddingModel` - TF-IDF fallback with financial vocabulary
   - `EmbeddingModelLoader` - Loads financial term lists
   - `SecEmbeddingSchemaFactory` - Convenience factory

#### ‚ö†Ô∏è What Needs Migration:
1. **Manual embedding calls** ‚Üí Metadata-driven
2. **Application-layer logic** ‚Üí Infrastructure-layer (AbstractGovDataDownloader)
3. **Dead code**: `EmbeddingService.java` (unused interface, 0 implementations)

---

### Migration Steps

#### 7.1 Add Schema-Level Configuration

**File**: `govdata/src/main/resources/sec-schema.json` (create or update)

**Add to schema root**:
```json
{
  "schemaName": "sec",
  "comment": "SEC EDGAR XBRL filings...",

  "embeddingCache": {
    "enabled": true,
    "path": "{cacheDir}/embeddings_cache",
    "provider": "onnx",
    "model": "all-MiniLM-L6-v2",
    "dimension": 384,
    "preprocessor": "org.apache.calcite.adapter.govdata.sec.SecTextVectorizer"
  },

  "partitionedTables": [...]
}
```

**New Field**: `preprocessor` - Optional class to preprocess text before embedding
- Allows SEC to use `SecTextVectorizer` for XBRL-specific chunking
- Called before `TextEmbeddingProvider.generateEmbedding()`

#### 7.2 Add Vector Columns to SEC Tables

**Update table definitions**:

**financial_line_items**:
```json
{
  "name": "financial_line_items",
  "columns": [
    {
      "name": "concept_label",
      "type": "string",
      "nullable": true,
      "comment": "Human-readable label for XBRL concept"
    },
    {
      "name": "concept_label_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumn": "concept_label",
        "provider": "onnx",
        "preprocessor": "conceptLabel"
      },
      "comment": "Vector embedding for semantic search on financial concepts"
    },
    {
      "name": "footnote_text",
      "type": "string",
      "nullable": true,
      "comment": "Associated footnote text"
    },
    {
      "name": "footnote_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumn": "footnote_text",
        "preprocessor": "footnote"
      },
      "comment": "Vector embedding for semantic search on footnotes"
    }
  ]
}
```

**md_and_a_sections**:
```json
{
  "name": "md_and_a_sections",
  "columns": [
    {
      "name": "section_text",
      "type": "string",
      "nullable": true,
      "comment": "MD&A section text"
    },
    {
      "name": "section_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumn": "section_text",
        "preprocessor": "mdaSection"
      },
      "comment": "Vector embedding for semantic search on MD&A sections"
    }
  ]
}
```

**risk_factors**:
```json
{
  "name": "risk_factors",
  "columns": [
    {
      "name": "risk_text",
      "type": "string",
      "nullable": true,
      "comment": "Risk factor description"
    },
    {
      "name": "risk_embedding",
      "type": "array<double>",
      "nullable": true,
      "computed": true,
      "embeddingConfig": {
        "sourceColumn": "risk_text",
        "preprocessor": "riskFactor"
      },
      "comment": "Vector embedding for semantic search on risk factors"
    }
  ]
}
```

#### 7.3 Enhance AbstractGovDataDownloader for Preprocessors

**File**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/AbstractGovDataDownloader.java`

**Update `generateEmbeddingColumns()` method** (from Phase 4):

```java
private void generateEmbeddingColumns(Map<String, Object> record,
    List<TableColumn> columns, TextEmbeddingProvider provider) {

  for (TableColumn column : columns) {
    if (!column.isComputed() || !column.hasEmbeddingConfig()) {
      continue;
    }

    String sourceColumn = column.getEmbeddingSourceColumn();
    Object sourceValue = record.get(sourceColumn);

    if (sourceValue instanceof String) {
      String text = (String) sourceValue;

      // NEW: Apply preprocessor if configured
      String preprocessorType = column.getEmbeddingPreprocessor();
      if (preprocessorType != null) {
        text = applyPreprocessor(text, preprocessorType, record);
      }

      try {
        double[] embedding = provider.generateEmbedding(text);
        record.put(column.getName(), embedding);

        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug("Generated embedding for column {} from {}: {} dimensions",
                      column.getName(), sourceColumn, embedding.length);
        }
      } catch (EmbeddingException e) {
        LOGGER.debug("Failed to generate embedding for column {}: {}",
                    column.getName(), e.getMessage());
        record.put(column.getName(), null);
      }
    } else {
      record.put(column.getName(), null);
    }
  }
}

/**
 * Apply domain-specific preprocessing before embedding generation.
 */
private String applyPreprocessor(String text, String preprocessorType, Map<String, Object> record) {
  // Get preprocessor from schema config or use default
  TextPreprocessor preprocessor = getTextPreprocessor(preprocessorType);
  if (preprocessor != null) {
    return preprocessor.preprocess(text, record);
  }
  return text;
}

/**
 * Get or create text preprocessor instance.
 */
private TextPreprocessor getTextPreprocessor(String type) {
  // Load from schema config "preprocessor" field
  // For SEC: returns SecTextVectorizer wrapper
  // For other schemas: may return identity preprocessor
  return preprocessorCache.computeIfAbsent(type, this::createPreprocessor);
}
```

#### 7.4 Create TextPreprocessor Interface

**File**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/TextPreprocessor.java` (new)

```java
package org.apache.calcite.adapter.govdata;

import java.util.Map;

/**
 * Preprocesses text before embedding generation.
 *
 * <p>Allows domain-specific text transformation:
 * - SEC: Extract semantic chunks from XBRL (SecTextVectorizer)
 * - Other: Identity transformation or custom logic
 */
public interface TextPreprocessor {

  /**
   * Preprocess text before embedding.
   *
   * @param text Source text to preprocess
   * @param context Full record context (for cross-column preprocessing)
   * @return Preprocessed text ready for embedding
   */
  String preprocess(String text, Map<String, Object> context);

  /**
   * Get preprocessor type identifier.
   */
  String getType();
}
```

#### 7.5 Adapt SecTextVectorizer to TextPreprocessor

**File**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/sec/SecTextPreprocessor.java` (new)

```java
package org.apache.calcite.adapter.govdata.sec;

import org.apache.calcite.adapter.govdata.TextPreprocessor;
import java.util.Map;

/**
 * Adapter that wraps SecTextVectorizer as a TextPreprocessor.
 */
public class SecTextPreprocessor implements TextPreprocessor {

  private final SecTextVectorizer vectorizer;
  private final String preprocessorType;

  public SecTextPreprocessor(SecTextVectorizer vectorizer, String type) {
    this.vectorizer = vectorizer;
    this.preprocessorType = type;
  }

  @Override
  public String preprocess(String text, Map<String, Object> context) {
    // Route to appropriate SecTextVectorizer method based on type
    switch (preprocessorType) {
      case "conceptLabel":
        return vectorizer.extractConceptContext(text, context);
      case "footnote":
        return vectorizer.extractFootnoteContext(text, context);
      case "mdaSection":
        return vectorizer.extractMdaContext(text, context);
      case "riskFactor":
        return vectorizer.extractRiskContext(text, context);
      default:
        return text; // No preprocessing
    }
  }

  @Override
  public String getType() {
    return "sec-" + preprocessorType;
  }
}
```

**Update SecTextVectorizer** with new extraction methods:

```java
// Add to SecTextVectorizer.java

/**
 * Extract semantic context for a concept label.
 */
public String extractConceptContext(String conceptLabel, Map<String, Object> record) {
  // Existing logic: create contextual chunk with concept group info
  // Return: "[CONTEXT: Revenue] [METRIC] Revenue [VALUE] ..."
  return buildConceptChunk(conceptLabel, record);
}

/**
 * Extract semantic context for a footnote.
 */
public String extractFootnoteContext(String footnoteText, Map<String, Object> record) {
  // Existing logic: associate footnote with referenced items
  return buildFootnoteChunk(footnoteText, record);
}

/**
 * Extract semantic context for MD&A section.
 */
public String extractMdaContext(String sectionText, Map<String, Object> record) {
  // Existing logic: chunk MD&A with section context
  return buildMdaChunk(sectionText, record);
}

/**
 * Extract semantic context for risk factor.
 */
public String extractRiskContext(String riskText, Map<String, Object> record) {
  // Existing logic: extract risk-related context
  return buildRiskChunk(riskText, record);
}
```

#### 7.6 Update TableColumn Class

**File**: `file/src/main/java/org/apache/calcite/adapter/file/partition/PartitionedTableConfig.java`

**Add to TableColumn** (extends Phase 4.1):

```java
public static class TableColumn {
  private final String name;
  private final String type;
  private final boolean nullable;
  private final String comment;

  // From Phase 4
  private final boolean computed;
  private final Map<String, Object> embeddingConfig;

  // NEW: Existing methods plus:
  public String getEmbeddingPreprocessor() {
    return embeddingConfig != null ?
        (String) embeddingConfig.get("preprocessor") : null;
  }
}
```

#### 7.7 Remove Dead Code

**Delete**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/sec/EmbeddingService.java`

**Reason**: Unused interface (0 implementations), superseded by `TextEmbeddingProvider`

**Verification**:
```bash
# Confirm no usages
grep -r "implements EmbeddingService" govdata/
grep -r "extends EmbeddingService" govdata/
# Both should return 0 results
```

#### 7.8 Update SEC Conversion Logic

**File**: `govdata/src/main/java/org/apache/calcite/adapter/govdata/sec/SecToParquetConverter.java` (or similar)

**Before** (Manual embedding):
```java
// Old approach - explicit embedding calls
SecTextVectorizer vectorizer = new SecTextVectorizer(384, config);
for (Map<String, Object> record : records) {
  String text = (String) record.get("concept_label");
  double[] embedding = vectorizer.vectorizeConceptLabel(text, record);
  record.put("concept_label_embedding", embedding);
}
```

**After** (Metadata-driven):
```java
// New approach - automatic via AbstractGovDataDownloader
// convertJsonRecordToTypedMap() handles embeddings based on schema
for (JsonNode recordNode : jsonRecords) {
  Map<String, Object> typedRecord = convertJsonRecordToTypedMap(
      recordNode, tableColumns, missingValueIndicator);
  // Embeddings already generated based on schema metadata!
  records.add(typedRecord);
}
```

#### 7.9 Testing

**Create**: `govdata/src/test/java/org/apache/calcite/adapter/govdata/sec/SecEmbeddingIntegrationTest.java`

```java
@Tag("integration")
@Test
void testFinancialLineItemsWithEmbeddings() throws Exception {
  // Load SEC schema with embedding config
  String modelJson = loadSecModelWithEmbeddings();

  // Download sample 10-K filing
  downloadSample10K("AAPL", "2023");

  // Query with embeddings
  String sql =
      "SELECT concept_label, concept_label_embedding " +
      "FROM sec.financial_line_items " +
      "WHERE cik = '0000320193' " + // Apple
      "LIMIT 1";

  try (ResultSet rs = executeQuery(sql)) {
    assertTrue(rs.next());
    String label = rs.getString("concept_label");
    Array embeddingArray = rs.getArray("concept_label_embedding");
    double[] embedding = (double[]) embeddingArray.getArray();

    assertEquals(384, embedding.length);
    assertNotNull(label);
  }
}

@Tag("integration")
@Test
void testSemanticSearchAcrossFinancialConcepts() throws Exception {
  downloadSample10K("AAPL", "2023");

  String sql =
      "WITH target AS (" +
      "  SELECT concept_label_embedding AS emb " +
      "  FROM sec.financial_line_items " +
      "  WHERE concept_label LIKE '%Revenue%' " +
      "  LIMIT 1" +
      ") " +
      "SELECT " +
      "  concept_label, " +
      "  COSINE_SIMILARITY(concept_label_embedding, target.emb) AS similarity " +
      "FROM sec.financial_line_items, target " +
      "ORDER BY similarity DESC " +
      "LIMIT 10";

  try (ResultSet rs = executeQuery(sql)) {
    int count = 0;
    while (rs.next()) {
      String label = rs.getString("concept_label");
      double similarity = rs.getDouble("similarity");
      assertTrue(similarity >= 0.0 && similarity <= 1.0);
      count++;
    }
    assertEquals(10, count);
  }
}

@Tag("integration")
@Test
void testPreprocessorIntegration() throws Exception {
  // Verify SecTextVectorizer preprocessing is applied
  downloadSample10K("AAPL", "2023");

  // Check that embeddings capture semantic context
  String sql =
      "SELECT concept_label, concept_label_embedding " +
      "FROM sec.financial_line_items " +
      "WHERE concept_label = 'Revenue' " +
      "LIMIT 1";

  // The embedding should include contextual information
  // added by SecTextVectorizer (e.g., "[CONTEXT: Revenue] [METRIC] Revenue...")
  // Verify by comparing similarity with related concepts
  try (ResultSet rs = executeQuery(sql)) {
    assertTrue(rs.next());
    // Embedding should be contextually enriched
    double[] embedding = (double[]) rs.getArray("concept_label_embedding").getArray();
    assertNotNull(embedding);
  }
}
```

---

### Migration Benefits

1. **Consistency**: SEC uses same metadata-driven approach as econ
2. **Maintainability**: No code changes needed for new vector columns
3. **Flexibility**: Easy to enable/disable embeddings via schema config
4. **Reusability**: `SecTextVectorizer` still used but as pluggable preprocessor
5. **Cache sharing**: SEC can use same persistent embedding cache as econ

### Backward Compatibility

**Keep existing SEC components**:
- `SecTextVectorizer` - Becomes preprocessor implementation
- `SecEmbeddingModel` - Remains as TF-IDF fallback
- `SecEmbeddingSchemaFactory` - Can be updated to use new approach

**Migration is non-breaking**:
- Existing SEC code continues to work
- New schemas can opt into metadata-driven approach
- Gradual migration possible

---

## Change Log

### 2025-11-09: Updated for Govdata Refactorings
- **Updated Phase 4.1**: Corrected TableColumn location to PartitionedTableConfig.java:177
- **Updated Phase 4.2**: Fixed schema loading pattern (use ResourceStream instead of schemaConfig field)
- **Updated Phase 4.2**: Updated convertJsonRecordToTypedMap() line reference (1382 not 1226)
- **Updated Phase 4.2**: Added method signature challenge and overload solution
- **Added**: Section on recent refactoring benefits
- **Removed**: Unnecessary EconSchemaFactory modifications (supervisor pattern handles it)
- **Updated**: Completion checklist with specific file locations
- **Added Phase 7**: SEC Schema Unification - migrate SEC from manual to metadata-driven embeddings
  - Analyzed current SEC embedding infrastructure (already uses unified `TextEmbeddingProvider`)
  - Documented migration path to metadata-driven approach
  - Added `TextPreprocessor` interface for domain-specific preprocessing
  - Preserves `SecTextVectorizer` as pluggable preprocessor
  - Identifies dead code: `EmbeddingService.java` (unused interface)